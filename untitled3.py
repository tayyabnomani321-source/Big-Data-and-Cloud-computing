# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f77mkD7rCDjdVzkTaJDZpqYWdqgyd1ST
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, rand, floor, round, concat, lit
from datetime import datetime, timedelta

# ----------------------------
# Initialize Spark session
# ----------------------------
spark = SparkSession.builder \
    .appName("RetailChain_BigData_Analytics") \
    .getOrCreate()

# ----------------------------
# Parameters
# ----------------------------
num_transactions = 100000
num_stores = 50
num_customers = 5000
categories = ['Electronics', 'Clothing', 'Home', 'Sports', 'Toys', 'Groceries']

start_date = datetime(2026, 1, 1)
end_date = datetime(2026, 2, 24)
total_days = (end_date - start_date).days + 1

# ----------------------------
# Generate base DataFrame with Spark
# ----------------------------
df = spark.range(1, num_transactions + 1).toDF("TransactionID")

# Random Store
df = df.withColumn("Store", concat(lit("Store_"), (floor(rand(seed=42) * num_stores) + 1).cast("int")))

# Random Customer
df = df.withColumn("CustomerID", concat(lit("Customer_"), (floor(rand(seed=99) * num_customers) + 1).cast("int")))

# Random Category (Spark-native approach using modulo)
from pyspark.sql.functions import expr
df = df.withColumn("Category", expr(f"array('{categories[0]}','{categories[1]}','{categories[2]}','{categories[3]}','{categories[4]}','{categories[5]}')[int(floor(rand() * {len(categories)}))]"))

# Random Date
df = df.withColumn("Date", expr(f"date_add('{start_date.strftime('%Y-%m-%d')}', int(floor(rand() * {total_days})))"))

# Random Amount between 5 and 2000
df = df.withColumn("Amount", round(rand(seed=1234) * 1995 + 5, 2))

# ----------------------------
# Show sample
# ----------------------------
df.show(10, truncate=False)

# ----------------------------
# Aggregations
# ----------------------------
print("Total Sales per Store:")
df.groupBy("Store").sum("Amount").withColumnRenamed("sum(Amount)", "Total_Sales").show(10)

print("Average Sales per Category:")
df.groupBy("Category").avg("Amount").withColumnRenamed("avg(Amount)", "Average_Sales").show()

print("Top 5 Customers by Total Spend:")
df.groupBy("CustomerID").sum("Amount").withColumnRenamed("sum(Amount)", "Total_Spend") \
    .orderBy(col("Total_Spend").desc()).show(5)

# Stop Spark session
spark.stop()